{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bc8f841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bs4 in c:\\users\\angelluv\\anaconda3\\lib\\site-packages (0.0.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\angelluv\\anaconda3\\lib\\site-packages (from bs4) (4.10.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\angelluv\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (2.2.1)\n",
      "Requirement already satisfied: requests in c:\\users\\angelluv\\anaconda3\\lib\\site-packages (2.26.0)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\angelluv\\anaconda3\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\angelluv\\anaconda3\\lib\\site-packages (from requests) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\angelluv\\anaconda3\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\angelluv\\anaconda3\\lib\\site-packages (from requests) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install bs4\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dbe4d9",
   "metadata": {},
   "source": [
    "1) Write a python program to display all the header tags from wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8178f9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n",
      "<h2>Navigation menu</h2>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-personal-label\">\n",
      "<span class=\"vector-menu-heading-label\">Personal tools</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-namespaces-label\">\n",
      "<span class=\"vector-menu-heading-label\">Namespaces</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-views-label\">\n",
      "<span class=\"vector-menu-heading-label\">Views</span>\n",
      "</h3>\n",
      "<h3>\n",
      "<label for=\"searchInput\">Search</label>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-navigation-label\">\n",
      "<span class=\"vector-menu-heading-label\">Navigation</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-interaction-label\">\n",
      "<span class=\"vector-menu-heading-label\">Contribute</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-tb-label\">\n",
      "<span class=\"vector-menu-heading-label\">Tools</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-coll-print_export-label\">\n",
      "<span class=\"vector-menu-heading-label\">Print/export</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-wikibase-otherprojects-label\">\n",
      "<span class=\"vector-menu-heading-label\">In other projects</span>\n",
      "</h3>\n",
      "<h3 class=\"vector-menu-heading\" id=\"p-lang-label\">\n",
      "<span class=\"vector-menu-heading-label\">Languages</span>\n",
      "</h3>\n"
     ]
    }
   ],
   "source": [
    "import  requests\n",
    "from  bs4 import BeautifulSoup\n",
    "\n",
    "url = requests.get('https://en.wikipedia.org/wiki/Main_Page')\n",
    "soup = BeautifulSoup(url.content)\n",
    "story = soup.find_all(['h1', 'h2','h3'])\n",
    "for i in story:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e580a6",
   "metadata": {},
   "source": [
    "2) Write a python program to display IMDB’s Top rated 100 movies’ data (i.e. name, rating, year of release)\n",
    "and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0251f978",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=requests.get(\"https://www.imdb.com/search/title/?groups=top_100&sort=user_rating,desc\")\n",
    "soup=BeautifulSoup(url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce21fc9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Shawshank Redemption',\n",
       " 'The Godfather',\n",
       " 'The Dark Knight',\n",
       " 'The Lord of the Rings: The Return of the King',\n",
       " \"Schindler's List\",\n",
       " 'The Godfather Part II',\n",
       " '12 Angry Men',\n",
       " 'Pulp Fiction',\n",
       " 'Inception',\n",
       " 'The Lord of the Rings: The Two Towers',\n",
       " 'Fight Club',\n",
       " 'The Lord of the Rings: The Fellowship of the Ring',\n",
       " 'Forrest Gump',\n",
       " 'Il buono, il brutto, il cattivo',\n",
       " 'The Matrix',\n",
       " 'Goodfellas',\n",
       " 'The Empire Strikes Back',\n",
       " \"One Flew Over the Cuckoo's Nest\",\n",
       " 'Interstellar',\n",
       " 'Cidade de Deus',\n",
       " 'Sen to Chihiro no kamikakushi',\n",
       " 'Saving Private Ryan',\n",
       " 'The Green Mile',\n",
       " 'La vita è bella',\n",
       " 'Se7en',\n",
       " 'Terminator 2: Judgment Day',\n",
       " 'The Silence of the Lambs',\n",
       " 'Star Wars',\n",
       " 'Seppuku',\n",
       " 'Shichinin no samurai',\n",
       " \"It's a Wonderful Life\",\n",
       " 'Gisaengchung',\n",
       " 'Whiplash',\n",
       " 'The Intouchables',\n",
       " 'The Prestige',\n",
       " 'The Departed',\n",
       " 'The Pianist',\n",
       " 'Gladiator',\n",
       " 'American History X',\n",
       " 'The Usual Suspects',\n",
       " 'Léon',\n",
       " 'The Lion King',\n",
       " 'Nuovo Cinema Paradiso',\n",
       " 'Hotaru no haka',\n",
       " 'Back to the Future',\n",
       " 'Apocalypse Now',\n",
       " 'Alien',\n",
       " 'Once Upon a Time in the West',\n",
       " 'Psycho',\n",
       " 'Rear Window']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_name=[]\n",
    "for i in soup.find_all('div',class_=\"lister-item\"):\n",
    "    for a in i.find_all('h3',class_=\"lister-item-header\"):\n",
    "        for b in a.find_all('a'):\n",
    "            movie_name.append(b.text)\n",
    "movie_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa77648c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['9.3',\n",
       " '9.2',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '9',\n",
       " '8.9',\n",
       " '8.8',\n",
       " '8.8',\n",
       " '8.8',\n",
       " '8.8',\n",
       " '8.8',\n",
       " '8.8',\n",
       " '8.7',\n",
       " '8.7',\n",
       " '8.7',\n",
       " '8.7',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.6',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5',\n",
       " '8.5']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings=[]\n",
    "for i in soup.find_all('div',class_=\"inline-block ratings-imdb-rating\"):\n",
    "    ratings.append(i.get('data-value'))\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c89f6d06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['(1994)',\n",
       " '(1972)',\n",
       " '(2008)',\n",
       " '(2003)',\n",
       " '(1993)',\n",
       " '(1974)',\n",
       " '(1957)',\n",
       " '(1994)',\n",
       " '(2010)',\n",
       " '(2002)',\n",
       " '(1999)',\n",
       " '(2001)',\n",
       " '(1994)',\n",
       " '(1966)',\n",
       " '(1999)',\n",
       " '(1990)',\n",
       " '(1980)',\n",
       " '(1975)',\n",
       " '(2014)',\n",
       " '(2002)',\n",
       " '(2001)',\n",
       " '(1998)',\n",
       " '(1999)',\n",
       " '(1997)',\n",
       " '(1995)',\n",
       " '(1991)',\n",
       " '(1991)',\n",
       " '(1977)',\n",
       " '(1962)',\n",
       " '(1954)',\n",
       " '(1946)',\n",
       " '(2019)',\n",
       " '(2014)',\n",
       " '(2011)',\n",
       " '(2006)',\n",
       " '(2006)',\n",
       " '(2002)',\n",
       " '(2000)',\n",
       " '(1998)',\n",
       " '(1995)',\n",
       " '(1994)',\n",
       " '(1994)',\n",
       " '(1988)',\n",
       " '(1988)',\n",
       " '(1985)',\n",
       " '(1979)',\n",
       " '(1979)',\n",
       " '(1968)',\n",
       " '(1960)',\n",
       " '(1954)']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_of_release=[]\n",
    "for i in soup.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "    year_of_release.append(i.text)\n",
    "year_of_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "84c6eeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 50 50\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_name),len(ratings),len(year_of_release))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "912df6ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>ratings</th>\n",
       "      <th>year_of_release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Shawshank Redemption</td>\n",
       "      <td>9.3</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The Godfather</td>\n",
       "      <td>9.2</td>\n",
       "      <td>(1972)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Dark Knight</td>\n",
       "      <td>9</td>\n",
       "      <td>(2008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Lord of the Rings: The Return of the King</td>\n",
       "      <td>9</td>\n",
       "      <td>(2003)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Schindler's List</td>\n",
       "      <td>9</td>\n",
       "      <td>(1993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Godfather Part II</td>\n",
       "      <td>9</td>\n",
       "      <td>(1974)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>12 Angry Men</td>\n",
       "      <td>9</td>\n",
       "      <td>(1957)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Pulp Fiction</td>\n",
       "      <td>8.9</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Inception</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(2010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The Lord of the Rings: The Two Towers</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Fight Club</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Lord of the Rings: The Fellowship of the Ring</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Forrest Gump</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Il buono, il brutto, il cattivo</td>\n",
       "      <td>8.8</td>\n",
       "      <td>(1966)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The Matrix</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Goodfellas</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1990)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The Empire Strikes Back</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>One Flew Over the Cuckoo's Nest</td>\n",
       "      <td>8.7</td>\n",
       "      <td>(1975)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Interstellar</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cidade de Deus</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Sen to Chihiro no kamikakushi</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(2001)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Saving Private Ryan</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>The Green Mile</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1999)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>La vita è bella</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1997)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Se7en</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Terminator 2: Judgment Day</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>The Silence of the Lambs</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1991)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Star Wars</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1977)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Seppuku</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1962)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Shichinin no samurai</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1954)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>It's a Wonderful Life</td>\n",
       "      <td>8.6</td>\n",
       "      <td>(1946)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Gisaengchung</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2019)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Whiplash</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>The Intouchables</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The Prestige</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>The Departed</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2006)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The Pianist</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Gladiator</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(2000)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>American History X</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1998)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>The Usual Suspects</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1995)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Léon</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>The Lion King</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1994)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>Nuovo Cinema Paradiso</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>Hotaru no haka</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1988)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>Back to the Future</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1985)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Apocalypse Now</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>Alien</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1979)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Once Upon a Time in the West</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1968)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>Psycho</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1960)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Rear Window</td>\n",
       "      <td>8.5</td>\n",
       "      <td>(1954)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           movie_name ratings year_of_release\n",
       "0                            The Shawshank Redemption     9.3          (1994)\n",
       "1                                       The Godfather     9.2          (1972)\n",
       "2                                     The Dark Knight       9          (2008)\n",
       "3       The Lord of the Rings: The Return of the King       9          (2003)\n",
       "4                                    Schindler's List       9          (1993)\n",
       "5                               The Godfather Part II       9          (1974)\n",
       "6                                        12 Angry Men       9          (1957)\n",
       "7                                        Pulp Fiction     8.9          (1994)\n",
       "8                                           Inception     8.8          (2010)\n",
       "9               The Lord of the Rings: The Two Towers     8.8          (2002)\n",
       "10                                         Fight Club     8.8          (1999)\n",
       "11  The Lord of the Rings: The Fellowship of the Ring     8.8          (2001)\n",
       "12                                       Forrest Gump     8.8          (1994)\n",
       "13                    Il buono, il brutto, il cattivo     8.8          (1966)\n",
       "14                                         The Matrix     8.7          (1999)\n",
       "15                                         Goodfellas     8.7          (1990)\n",
       "16                            The Empire Strikes Back     8.7          (1980)\n",
       "17                    One Flew Over the Cuckoo's Nest     8.7          (1975)\n",
       "18                                       Interstellar     8.6          (2014)\n",
       "19                                     Cidade de Deus     8.6          (2002)\n",
       "20                      Sen to Chihiro no kamikakushi     8.6          (2001)\n",
       "21                                Saving Private Ryan     8.6          (1998)\n",
       "22                                     The Green Mile     8.6          (1999)\n",
       "23                                    La vita è bella     8.6          (1997)\n",
       "24                                              Se7en     8.6          (1995)\n",
       "25                         Terminator 2: Judgment Day     8.6          (1991)\n",
       "26                           The Silence of the Lambs     8.6          (1991)\n",
       "27                                          Star Wars     8.6          (1977)\n",
       "28                                            Seppuku     8.6          (1962)\n",
       "29                               Shichinin no samurai     8.6          (1954)\n",
       "30                              It's a Wonderful Life     8.6          (1946)\n",
       "31                                       Gisaengchung     8.5          (2019)\n",
       "32                                           Whiplash     8.5          (2014)\n",
       "33                                   The Intouchables     8.5          (2011)\n",
       "34                                       The Prestige     8.5          (2006)\n",
       "35                                       The Departed     8.5          (2006)\n",
       "36                                        The Pianist     8.5          (2002)\n",
       "37                                          Gladiator     8.5          (2000)\n",
       "38                                 American History X     8.5          (1998)\n",
       "39                                 The Usual Suspects     8.5          (1995)\n",
       "40                                               Léon     8.5          (1994)\n",
       "41                                      The Lion King     8.5          (1994)\n",
       "42                              Nuovo Cinema Paradiso     8.5          (1988)\n",
       "43                                     Hotaru no haka     8.5          (1988)\n",
       "44                                 Back to the Future     8.5          (1985)\n",
       "45                                     Apocalypse Now     8.5          (1979)\n",
       "46                                              Alien     8.5          (1979)\n",
       "47                       Once Upon a Time in the West     8.5          (1968)\n",
       "48                                             Psycho     8.5          (1960)\n",
       "49                                        Rear Window     8.5          (1954)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'movie_name':movie_name,'ratings':ratings,'year_of_release':year_of_release})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac98310",
   "metadata": {},
   "source": [
    "3) Write a python program to display IMDB’s Top rated 100 Indian movies’ data (i.e. name, rating, year of\n",
    "release) and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e84f906a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=requests.get('https://www.imdb.com/list/ls009997493/')\n",
    "soup=BeautifulSoup(url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ca90560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Rang De Basanti',\n",
       " '3 Idiots',\n",
       " 'Taare Zameen Par',\n",
       " 'Dil Chahta Hai',\n",
       " 'Swades: We, the People',\n",
       " 'Lagaan: Once Upon a Time in India',\n",
       " 'Gangs of Wasseypur',\n",
       " 'Barfi!',\n",
       " 'Anand',\n",
       " 'Munna Bhai M.B.B.S.',\n",
       " 'A Wednesday',\n",
       " 'Andaz Apna Apna',\n",
       " 'Sholay',\n",
       " 'Bhaag Milkha Bhaag',\n",
       " 'Hera Pheri',\n",
       " 'Udaan',\n",
       " 'Kahaani',\n",
       " 'Black',\n",
       " 'Chak De! India',\n",
       " 'Khosla Ka Ghosla!',\n",
       " 'Jo Jeeta Wohi Sikandar',\n",
       " 'Zindagi Na Milegi Dobara',\n",
       " 'Paan Singh Tomar',\n",
       " 'Dilwale Dulhania Le Jayenge',\n",
       " 'Omkara',\n",
       " 'Lage Raho Munna Bhai',\n",
       " 'Iqbal',\n",
       " 'The Lunchbox',\n",
       " 'Black Friday',\n",
       " 'Company',\n",
       " 'Golmaal',\n",
       " 'Dev.D',\n",
       " 'Jaane Bhi Do Yaaro',\n",
       " 'OMG: Oh My God!',\n",
       " 'Mughal-E-Azam',\n",
       " 'Gulaal',\n",
       " 'Dor',\n",
       " 'Jab We Met',\n",
       " 'Pyaasa',\n",
       " 'The Legend of Bhagat Singh',\n",
       " 'Masoom',\n",
       " 'Salaam Bombay!',\n",
       " 'Satya',\n",
       " 'Vicky Donor',\n",
       " 'Lakshya',\n",
       " 'Vaastav: The Reality',\n",
       " 'Kal Ho Naa Ho',\n",
       " 'Oye Lucky! Lucky Oye!',\n",
       " 'Sarfarosh',\n",
       " 'Gangaajal',\n",
       " 'Angoor',\n",
       " 'Madras Cafe',\n",
       " 'English Vinglish',\n",
       " 'Chupke Chupke',\n",
       " 'Johnny Gaddaar',\n",
       " 'Maqbool',\n",
       " 'Hazaaron Khwaishein Aisi',\n",
       " 'Rock On!!',\n",
       " 'Don',\n",
       " 'Chhoti Si Baat',\n",
       " 'Guide',\n",
       " 'Raanjhanaa',\n",
       " 'Deewaar',\n",
       " 'Special Chabbis',\n",
       " 'Padosan',\n",
       " 'Mumbai Meri Jaan',\n",
       " 'Ab Tak Chhappan',\n",
       " 'Kai po che!',\n",
       " 'Awaara',\n",
       " 'Shree 420',\n",
       " 'Earth',\n",
       " 'Gunda',\n",
       " 'Parinda',\n",
       " 'Dasvidaniya',\n",
       " 'Hey Ram',\n",
       " 'Pinjar: Beyond Boundaries...',\n",
       " 'Socha Na Tha',\n",
       " 'Guru',\n",
       " 'Bawarchi',\n",
       " 'Manorama: Six Feet Under',\n",
       " 'Mr. India',\n",
       " 'Aamir',\n",
       " 'Zakhm',\n",
       " 'Water',\n",
       " 'Stanley Ka Dabba',\n",
       " 'Agneepath',\n",
       " 'My Name Is Khan',\n",
       " 'Qayamat Se Qayamat Tak',\n",
       " '3 Deewarein',\n",
       " 'Abhimaan',\n",
       " 'Sarkar',\n",
       " 'Bheja Fry',\n",
       " 'Mother India',\n",
       " 'Jaane Tu... Ya Jaane Na',\n",
       " 'Delhi Belly',\n",
       " 'Wake Up Sid',\n",
       " 'Rangeela',\n",
       " 'Shatranj Ke Khilari',\n",
       " 'Pyaar Ka Punchnama',\n",
       " 'Ek Hasina Thi']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movie_name=[]\n",
    "for i in soup.find_all('div',class_=\"lister-item-content\"):\n",
    "    for a in i.find_all('h3',class_=\"lister-item-header\"):\n",
    "        for b in a.find_all('a'):\n",
    "            movie_name.append(b.text)\n",
    "movie_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "769601d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['8.1',\n",
       " '8.4',\n",
       " '8.3',\n",
       " '8.1',\n",
       " '8.2',\n",
       " '8.1',\n",
       " '8.2',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '8',\n",
       " '8.1',\n",
       " '8.2',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '8.3',\n",
       " '8.2',\n",
       " '8.2',\n",
       " '8.2',\n",
       " '8',\n",
       " '8.1',\n",
       " '8',\n",
       " '8.1',\n",
       " '7.8',\n",
       " '8.4',\n",
       " '8',\n",
       " '8.5',\n",
       " '7.9',\n",
       " '8.3',\n",
       " '8.1',\n",
       " '8.1',\n",
       " '8',\n",
       " '7.9',\n",
       " '7.9',\n",
       " '8.3',\n",
       " '8.1',\n",
       " '8.4',\n",
       " '7.9',\n",
       " '8.3',\n",
       " '7.8',\n",
       " '7.9',\n",
       " '8',\n",
       " '7.9',\n",
       " '7.7',\n",
       " '8.1',\n",
       " '7.8',\n",
       " '8.3',\n",
       " '7.6',\n",
       " '7.8',\n",
       " '8.3',\n",
       " '7.9',\n",
       " '8',\n",
       " '7.9',\n",
       " '7.7',\n",
       " '7.7',\n",
       " '8.3',\n",
       " '8.4',\n",
       " '7.6',\n",
       " '8',\n",
       " '8',\n",
       " '8',\n",
       " '7.7',\n",
       " '7.8',\n",
       " '7.8',\n",
       " '7.8',\n",
       " '7.9',\n",
       " '7.6',\n",
       " '7.3',\n",
       " '7.8',\n",
       " '7.8',\n",
       " '7.9',\n",
       " '7.9',\n",
       " '7.4',\n",
       " '7.7',\n",
       " '8.1',\n",
       " '7.6',\n",
       " '7.7',\n",
       " '7.6',\n",
       " '7.9',\n",
       " '7.7',\n",
       " '7.8',\n",
       " '7.6',\n",
       " '7.9',\n",
       " '7.4',\n",
       " '7.8',\n",
       " '7.8',\n",
       " '7.6',\n",
       " '7.6',\n",
       " '7.8',\n",
       " '7.4',\n",
       " '7.5',\n",
       " '7.6',\n",
       " '7.4',\n",
       " '7.5',\n",
       " '7.6',\n",
       " '7.5']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings=[]\n",
    "for i in soup.find_all('div',class_=\"ipl-rating-widget\"):\n",
    "    for a in i.find_all('div',class_=\"ipl-rating-star small\"):\n",
    "        ratings.append(a.text.strip('\\n'))\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff2cd4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2006',\n",
       " '2009',\n",
       " '2007',\n",
       " '2001',\n",
       " '2004',\n",
       " '2001',\n",
       " '2012',\n",
       " '2012',\n",
       " '1971',\n",
       " '2003',\n",
       " '2008',\n",
       " '1994',\n",
       " '1975',\n",
       " '2013',\n",
       " '2000',\n",
       " '2010',\n",
       " '2012',\n",
       " '2005',\n",
       " '2007',\n",
       " '2006',\n",
       " '1992',\n",
       " '2011',\n",
       " '2012',\n",
       " '1995',\n",
       " '2006',\n",
       " '2006',\n",
       " '2005',\n",
       " '2013',\n",
       " '2004',\n",
       " '2002',\n",
       " '1979',\n",
       " '2009',\n",
       " '1983',\n",
       " '2012',\n",
       " '1960',\n",
       " '2009',\n",
       " '2006',\n",
       " '2007',\n",
       " '1957',\n",
       " '2002',\n",
       " '1983',\n",
       " '1988',\n",
       " '1998',\n",
       " '2012',\n",
       " '2004',\n",
       " '1999',\n",
       " '2003',\n",
       " '2008',\n",
       " '1999',\n",
       " '2003',\n",
       " '1982',\n",
       " '2013',\n",
       " '2012',\n",
       " '1975',\n",
       " '2007',\n",
       " '2003',\n",
       " '2003',\n",
       " '2008',\n",
       " '1978',\n",
       " '1976',\n",
       " '1965',\n",
       " '2013',\n",
       " '1975',\n",
       " '2013',\n",
       " '1968',\n",
       " '2008',\n",
       " '2004',\n",
       " '2013',\n",
       " '1951',\n",
       " '1955',\n",
       " '1998',\n",
       " '1998',\n",
       " '1989',\n",
       " '2008',\n",
       " '2000',\n",
       " '2003',\n",
       " '2005',\n",
       " '2007',\n",
       " '1972',\n",
       " '2007',\n",
       " '1987',\n",
       " '2008',\n",
       " '1998',\n",
       " '2005',\n",
       " '2011',\n",
       " '1990',\n",
       " '2010',\n",
       " '1988',\n",
       " '2003',\n",
       " '1973',\n",
       " '2005',\n",
       " '2007',\n",
       " '1957',\n",
       " '2008',\n",
       " '2011',\n",
       " '2009',\n",
       " '1995',\n",
       " '1977',\n",
       " '2011',\n",
       " '2004']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "year_of_release=[]\n",
    "for i in soup.find_all('span',class_=\"lister-item-year text-muted unbold\"):\n",
    "    year_of_release.append(i.text.strip('(I) '))\n",
    "year_of_release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb02b003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 100 100\n"
     ]
    }
   ],
   "source": [
    "print(len(movie_name),len(ratings),len(year_of_release))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60b587d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_name</th>\n",
       "      <th>ratings</th>\n",
       "      <th>year_of_release</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Rang De Basanti</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3 Idiots</td>\n",
       "      <td>8.4</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Taare Zameen Par</td>\n",
       "      <td>8.3</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Dil Chahta Hai</td>\n",
       "      <td>8.1</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Swades: We, the People</td>\n",
       "      <td>8.2</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Wake Up Sid</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Rangeela</td>\n",
       "      <td>7.4</td>\n",
       "      <td>1995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Shatranj Ke Khilari</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Pyaar Ka Punchnama</td>\n",
       "      <td>7.6</td>\n",
       "      <td>2011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Ek Hasina Thi</td>\n",
       "      <td>7.5</td>\n",
       "      <td>2004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                movie_name ratings year_of_release\n",
       "0          Rang De Basanti     8.1            2006\n",
       "1                 3 Idiots     8.4            2009\n",
       "2         Taare Zameen Par     8.3            2007\n",
       "3           Dil Chahta Hai     8.1            2001\n",
       "4   Swades: We, the People     8.2            2004\n",
       "..                     ...     ...             ...\n",
       "95             Wake Up Sid     7.6            2009\n",
       "96                Rangeela     7.4            1995\n",
       "97     Shatranj Ke Khilari     7.5            1977\n",
       "98      Pyaar Ka Punchnama     7.6            2011\n",
       "99           Ek Hasina Thi     7.5            2004\n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({'movie_name':movie_name,'ratings':ratings,'year_of_release':year_of_release})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e70f28",
   "metadata": {},
   "source": [
    "4) Write s python program to display list of respected former presidents of India(i.e. Name , Term of office)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98494bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=requests.get('https://presidentofindia.nic.in/former-presidents.htm')\n",
    "soup=BeautifulSoup(url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "214cab2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shri Ram Nath Kovind',\n",
       " 'Shri Pranab Mukherjee',\n",
       " 'Smt Pratibha Devisingh Patil',\n",
       " 'DR. A.P.J. Abdul Kalam',\n",
       " 'Shri K. R. Narayanan',\n",
       " 'Dr Shankar Dayal Sharma',\n",
       " 'Shri R Venkataraman',\n",
       " 'Giani Zail Singh',\n",
       " 'Shri Neelam Sanjiva Reddy',\n",
       " 'Dr. Fakhruddin Ali Ahmed',\n",
       " 'Shri Varahagiri Venkata Giri',\n",
       " 'Dr. Zakir Husain',\n",
       " 'Dr. Sarvepalli Radhakrishnan',\n",
       " 'Dr. Rajendra Prasad']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "presidents_name=[]\n",
    "for i in soup.find_all(\"h3\"):\n",
    "    presidents_name.append(i.text.split('(')[0].strip())\n",
    "presidents_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ff23b2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['25 July, 2017 to 25 July, 2022',\n",
       " '25 July, 2012 to 25 July, 2017',\n",
       " '25 July, 2007 to 25 July, 2012',\n",
       " '25 July, 2002 to 25 July, 2007',\n",
       " '25 July, 1997 to 25 July, 2002',\n",
       " '25 July, 1992 to 25 July, 1997',\n",
       " '25 July, 1987 to 25 July, 1992',\n",
       " '25 July, 1982 to 25 July, 1987',\n",
       " '25 July, 1977 to 25 July, 1982',\n",
       " '24 August, 1974 to 11 February, 1977',\n",
       " '3 May, 1969 to 20 July, 1969 and 24 August, 1969 to 24 August, 1974',\n",
       " '3 May, 1967 to 3 May, 1969',\n",
       " '3 May, 1962 to 13 May, 1967',\n",
       " '26 January, 1950 to 13 May, 1962',\n",
       " '22-July-2022']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_of_office=[]\n",
    "for i in soup.find_all(\"p\"):\n",
    "    if len(i.find_all('span')) != 0:\n",
    "        term_of_office.append(i.text.split('|')[0].strip('Term of Office:').strip('Page last updated on:').strip('\\n').strip('15:58 PM'))\n",
    "term_of_office"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b2c7fb",
   "metadata": {},
   "source": [
    "5) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team and rating.\n",
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936da697",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9fce16e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'England', '27', '3,226', '119'], ['2', 'New Zealand', '22', '2,508', '114'], ['3', 'India', '31', '3,447', '111'], ['4', 'Pakistan', '22', '2,354', '107'], ['5', 'Australia', '29', '3,071', '106'], ['6', 'South Africa', '21', '2,111', '101'], ['7', 'Bangladesh', '30', '2,753', '92'], ['8', 'Sri Lanka', '29', '2,658', '92'], ['9', 'West Indies', '41', '2,902', '71'], ['10', 'Afghanistan', '18', '1,238', '69']]\n"
     ]
    }
   ],
   "source": [
    "url = requests.get('https://www.icc-cricket.com/rankings/mens/team-rankings/odi')\n",
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "\n",
    "teamsList = []\n",
    "\n",
    "odiTeamRanks = soup.find(\"div\", class_=\"rankings-block__container full rankings-table\")\n",
    "\n",
    "firstTeamHtml = odiTeamRanks.find(\"tr\", class_=\"rankings-block__banner\")\n",
    "\n",
    "if firstTeamHtml:\n",
    "    firstTeam = [firstTeamHtml.find(\"td\", class_=\"rankings-block__banner--pos\").text.strip(),\n",
    "                 firstTeamHtml.find(\"span\", class_=\"u-hide-phablet\").text.strip(),\n",
    "                 firstTeamHtml.find(\"td\", class_=\"rankings-block__banner--matches\").text.strip(),\n",
    "                 firstTeamHtml.find(\"td\", class_=\"rankings-block__banner--points\").text.strip(),\n",
    "                 firstTeamHtml.find(\"td\", class_=\"rankings-block__banner--rating u-text-right\").text.strip()]\n",
    "    teamsList.append(firstTeam)\n",
    "\n",
    "for team in odiTeamRanks.find_all('tr', class_=\"table-body\")[:9]:\n",
    "    teamRow = [team.find(\"td\", class_=\"table-body__cell table-body__cell--position u-text-right\").text.strip(),\n",
    "               team.find(\"span\", class_=\"u-hide-phablet\").text.strip()]\n",
    "    matchAndPoints = team.find_all(\"td\", class_=\"table-body__cell u-center-text\")\n",
    "    teamRow.append(matchAndPoints[0].text.strip())\n",
    "    teamRow.append(matchAndPoints[1].text.strip())\n",
    "    teamRow.append(team.find(\"td\", class_=\"rating\").text.strip())\n",
    "    teamsList.append(teamRow)\n",
    "\n",
    "print(teamsList)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa528af",
   "metadata": {},
   "source": [
    "b) Top 10 ODI Batsmen along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cdc481c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'Babar Azam', ['PAK', '890'], '890'], ['2', 'Rassie van der Dussen', 'SA', '789'], ['3', 'Quinton de Kock', 'SA', '784'], ['4', 'Imam-ul-Haq', 'PAK', '779'], ['5', 'Virat Kohli', 'IND', '744'], ['6', 'Rohit Sharma', 'IND', '740'], ['7', 'Jonny Bairstow', 'ENG', '732'], ['8', 'David Warner', 'AUS', '725'], ['9', 'Ross Taylor', 'NZ', '701'], ['10', 'Steve Smith', 'AUS', '697']]\n"
     ]
    }
   ],
   "source": [
    "url = requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')\n",
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "\n",
    "odiPlayers = []\n",
    "firstPlayerHtml = soup.find(\"div\", class_=\"rankings-block__top-player\")\n",
    "\n",
    "player1 = [firstPlayerHtml.find(\"span\", class_=\"rankings-block__pos-number\").text.strip(),\n",
    "           firstPlayerHtml.find(\"div\", class_=\"rankings-block__banner--name\").text.strip(),\n",
    "           firstPlayerHtml.find(\"div\", class_=\"rankings-block__banner--nationality\").text.strip().split(\"\\n\"),\n",
    "           firstPlayerHtml.find('div',class_=\"rankings-block__banner--rating\").text.strip()]\n",
    "\n",
    "odiPlayers.append(player1)\n",
    "\n",
    "for playerHtml in soup.find(\"div\", class_=\"rankings-block__container\").findAll(\"tr\", class_=\"table-body\"):\n",
    "    odiPlayers.append([playerHtml.find(\"span\", class_=\"rankings-table__pos-number\").text.strip(),\n",
    "                       playerHtml.find(\"td\", class_=\"table-body__cell name\").find(\"a\").text.strip(),\n",
    "                       playerHtml.find(\"span\", class_=\"table-body__logo-text\").text.strip(),\n",
    "                       playerHtml.find(\"td\", class_=\"table-body__cell u-text-right rating\").text.strip()])\n",
    "\n",
    "print(odiPlayers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ef884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "089f5827",
   "metadata": {},
   "source": [
    "c) Top 10 ODI bowlers along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74f31d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', 'Trent Boult', 'NZ', '775'],\n",
       " ['2', 'Josh Hazlewood', 'AUS', '718'],\n",
       " ['3', 'Mujeeb Ur Rahman', 'AFG', '676'],\n",
       " ['4', 'Jasprit Bumrah', 'IND', '662'],\n",
       " ['5', 'Shaheen Afridi', 'PAK', '661'],\n",
       " ['6', 'Mohammad Nabi', 'AFG', '657'],\n",
       " ['7', 'Mehedi Hasan', 'BAN', '655'],\n",
       " ['8', 'Matt Henry', 'NZ', '654'],\n",
       " ['9', 'Mitchell Starc', 'AUS', '653'],\n",
       " ['10', 'Rashid Khan', 'AFG', '651']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url=requests.get('https://www.icc-cricket.com/rankings/mens/player-rankings/odi')\n",
    "soup=BeautifulSoup(url.content,'html.parser')\n",
    "\n",
    "odiBowlers=[]\n",
    "firstbowlerHtml=soup.find('div', attrs={'data-cricket-role': 'bowling'})\n",
    "\n",
    "player1=[firstbowlerHtml.find('span',class_=\"rankings-block__pos-number\").text.strip(),\n",
    "                             firstbowlerHtml.find('div',class_=\"rankings-block__banner--name\").text.strip(),\n",
    "                             firstbowlerHtml.find('div',class_=\"rankings-block__banner--nationality\").text.strip('\\n775'),\n",
    "                             firstbowlerHtml.find('div',class_=\"rankings-block__banner--rating\").text.strip()]\n",
    "odiBowlers.append(player1)\n",
    "\n",
    "for playerHtml in firstbowlerHtml.findAll('tr',class_=\"table-body\"):\n",
    "    odiBowlers.append([playerHtml.find('span',class_=\"rankings-table__pos-number\").text.strip(),\n",
    "                       playerHtml.find('td',class_=\"table-body__cell name\").find('a').text.strip(),\n",
    "                       playerHtml.find('span',class_=\"table-body__logo-text\").text.strip(),\n",
    "                       playerHtml.find('td',class_=\"table-body__cell u-text-right rating\").text.strip()])\n",
    "\n",
    "odiBowlers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01314a0b",
   "metadata": {},
   "source": [
    "6) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape:\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5dda96",
   "metadata": {},
   "source": [
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac53587c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'Australia', '29', '4,837', '167'], ['2', 'England', '34', '4,097', '121'], ['3', 'South Africa', '35', '4,157', '119'], ['4', 'India', '33', '3,392', '103'], ['5', 'New Zealand', '32', '3,161', '99'], ['6', 'West Indies', '31', '2,815', '91'], ['7', 'Bangladesh', '12', '930', '78'], ['8', 'Pakistan', '30', '1,962', '65'], ['9', 'Ireland', '11', '516', '47'], ['10', 'Sri Lanka', '11', '495', '45']]\n"
     ]
    }
   ],
   "source": [
    "url = requests.get('https://www.icc-cricket.com/rankings/womens/team-rankings/odi')\n",
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "\n",
    "teamsList = []\n",
    "\n",
    "odiTeamRanks = soup.find(\"div\", class_=\"rankings-block__container full rankings-table\")\n",
    "\n",
    "firstTeamHtml = odiTeamRanks.find(\"tr\", class_=\"rankings-block__banner\")\n",
    "\n",
    "if firstTeamHtml:\n",
    "    firstTeam = [firstTeamHtml.find(\"td\", class_=\"rankings-block__banner--pos\").text.strip(),\n",
    "                 firstTeamHtml.find(\"span\", class_=\"u-hide-phablet\").text.strip(),\n",
    "                 firstTeamHtml.find(\"td\", class_=\"rankings-block__banner--matches\").text.strip(),\n",
    "                 firstTeamHtml.find(\"td\", class_=\"rankings-block__banner--points\").text.strip(),\n",
    "                 firstTeamHtml.find(\"td\", class_=\"rankings-block__banner--rating u-text-right\").text.strip()]\n",
    "    teamsList.append(firstTeam)\n",
    "\n",
    "for team in odiTeamRanks.find_all('tr', class_=\"table-body\")[:9]:\n",
    "    teamRow = [team.find(\"td\", class_=\"table-body__cell table-body__cell--position u-text-right\").text.strip(),\n",
    "               team.find(\"span\", class_=\"u-hide-phablet\").text.strip()]\n",
    "    matchAndPoints = team.find_all(\"td\", class_=\"table-body__cell u-center-text\")\n",
    "    teamRow.append(matchAndPoints[0].text.strip())\n",
    "    teamRow.append(matchAndPoints[1].text.strip())\n",
    "    teamRow.append(team.find(\"td\", class_=\"rating\").text.strip())\n",
    "    teamsList.append(teamRow)\n",
    "\n",
    "print(teamsList)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372eeba",
   "metadata": {},
   "source": [
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c5c509a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'Alyssa Healy', 'AUS', '785'], ['2', 'Beth Mooney', 'AUS', '749'], ['3', 'Natalie Sciver', 'ENG', '740'], ['4', 'Laura Wolvaardt', 'SA', '732'], ['5', 'Meg Lanning', 'AUS', '710'], ['6', 'Rachael Haynes', 'AUS', '701'], ['7', 'Smriti Mandhana', 'IND', '698'], ['8', 'Amy Satterthwaite', 'NZ', '681'], ['9', 'Harmanpreet Kaur', 'IND', '662'], ['10', 'Chamari Athapaththu', 'SL', '655']]\n"
     ]
    }
   ],
   "source": [
    "url = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')\n",
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "\n",
    "odiPlayers = []\n",
    "\n",
    "batsmenHtml = soup.find(\"div\", attrs={'class': 'rankings-block__container', 'data-cricket-role': 'batting'})\n",
    "\n",
    "firstPlayerHtml = batsmenHtml.find(\"div\", class_=\"rankings-block__top-player\")\n",
    "\n",
    "player1 = [firstPlayerHtml.find(\"span\", class_=\"rankings-block__pos-number\").text.strip(),\n",
    "           firstPlayerHtml.find(\"div\", class_=\"rankings-block__banner--name\").text.strip()]\n",
    "countryRating = firstPlayerHtml.find(\"div\", class_=\"rankings-block__banner--nationality\").text.strip().split(\"\\n\")\n",
    "player1.append(countryRating[0])\n",
    "player1.append(countryRating[1])\n",
    "odiPlayers.append(player1)\n",
    "\n",
    "for playerHtml in batsmenHtml.findAll(\"tr\", class_=\"table-body\"):\n",
    "    odiPlayers.append([playerHtml.find(\"span\", class_=\"rankings-table__pos-number\").text.strip(),\n",
    "                       playerHtml.find(\"td\", class_=\"table-body__cell name\").find(\"a\").text.strip(),\n",
    "                       playerHtml.find(\"span\", class_=\"table-body__logo-text\").text.strip(),\n",
    "                       playerHtml.find(\"td\", class_=\"table-body__cell u-text-right rating\").text.strip()])\n",
    "\n",
    "print(odiPlayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d489aa",
   "metadata": {},
   "source": [
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "637cb339",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1', 'Ellyse Perry', 'AUS', '374'], ['2', 'Natalie Sciver', 'ENG', '372'], ['3', 'Marizanne Kapp', 'SA', '349'], ['4', 'Hayley Matthews', 'WI', '339'], ['5', 'Amelia Kerr', 'NZ', '336'], ['6', 'Deepti Sharma', 'IND', '271'], ['7', 'Ashleigh Gardner', 'AUS', '270'], ['8', 'Jess Jonassen', 'AUS', '246'], ['9', 'Jhulan Goswami', 'IND', '219'], ['10', 'Sophie Ecclestone', 'ENG', '217']]\n"
     ]
    }
   ],
   "source": [
    "url = requests.get('https://www.icc-cricket.com/rankings/womens/player-rankings/odi')\n",
    "soup = BeautifulSoup(url.content, 'html.parser')\n",
    "\n",
    "odiAllRoundersPlayers = []\n",
    "\n",
    "allRounderHtml = soup.find(\"div\",\n",
    "                           attrs={'class': 'rankings-block__container', 'data-cricket-role': 'all_round'})\n",
    "\n",
    "firstPlayerHtml = allRounderHtml.find(\"div\", class_=\"rankings-block__top-player\")\n",
    "\n",
    "player1 = [firstPlayerHtml.find(\"span\", class_=\"rankings-block__pos-number\").text.strip(),\n",
    "           firstPlayerHtml.find(\"div\", class_=\"rankings-block__banner--name\").text.strip()]\n",
    "countryRating = firstPlayerHtml.find(\"div\", class_=\"rankings-block__banner--nationality\").text.strip().split(\"\\n\")\n",
    "player1.append(countryRating[0])\n",
    "player1.append(countryRating[1])\n",
    "odiAllRoundersPlayers.append(player1)\n",
    "\n",
    "for playerHtml in allRounderHtml.findAll(\"tr\", class_=\"table-body\"):\n",
    "    odiAllRoundersPlayers.append([playerHtml.find(\"span\", class_=\"rankings-table__pos-number\").text.strip(),\n",
    "                                  playerHtml.find(\"td\", class_=\"table-body__cell name\").find(\"a\").text.strip(),\n",
    "                                  playerHtml.find(\"span\", class_=\"table-body__logo-text\").text.strip(),\n",
    "                                  playerHtml.find(\"td\", class_=\"table-body__cell u-text-right rating\").text.strip()])\n",
    "\n",
    "print(odiAllRoundersPlayers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a86e4f6",
   "metadata": {},
   "source": [
    "7) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world :\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c8ce8c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=requests.get('https://www.cnbc.com/world/?region=world')\n",
    "soup=BeautifulSoup(url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6fdfb87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 rules for a successful open relationship, according to an NYC therapist',\n",
       " 'Biden administration awards $1.5 billion to fight opioid crisis',\n",
       " 'Top 10 cities with the best pizzerias worldwide—see where NYC lands on the list',\n",
       " 'Black Girls in Trader Joe’s creator shares her top five favorite products',\n",
       " \"Want to raise strong, resilient kids? Create 'nurturing routines,' says parenting expert—here’s how\",\n",
       " 'Why the airline climate change plan is trailing autos and EVs',\n",
       " \"'Queer Eye's Karamo Brown on the morning routines that keep him motivated\",\n",
       " 'These 7 states have the least air pollution in the U.S.',\n",
       " 'New York is now No. 1 port in a tipping point for U.S.-bound trade',\n",
       " \"Analysts have 'high conviction' that these stocks have major upside.\",\n",
       " 'Feared stock market bottom retest is now underway',\n",
       " \"The No. 1 best city to retire isn't in Florida but the runner up is\",\n",
       " 'The perils and promise of quantum computing are nearing. Here are ways to invest',\n",
       " 'Everything parents need to know about student loan forgiveness ',\n",
       " 'Check in, smoke up and tune out: Cannabis-friendly vacation rentals are catching on',\n",
       " 'Convertibles drive into the sunset as automakers invest in electric vehicles',\n",
       " \"Britain's lurch toward 'Reaganomics' gets a thumbs down from the markets\",\n",
       " \"Here's our plan for Monday after another painful week to own stocks\",\n",
       " 'What to watch in the markets in the week ahead',\n",
       " \"Pro Picks: Watch all of Friday's big stock calls on CNBC\",\n",
       " '13 careers where over 50% of workers are happy with their pay',\n",
       " \"New York AG wrongly said Yankees game on Apple TV+ costs extra — but it's free\",\n",
       " 'Tech stocks just had the worst two-week stretch since the start of the pandemic',\n",
       " \"Takeaways from Jim Cramer's interviews with the CEOs of Salesforce and Slack \",\n",
       " \"Some homebuyers are facing mortgage 'payment shock.' Here are ways to save\",\n",
       " 'Ether is down almost 20% since the merge. Here’s what’s going on',\n",
       " 'Elon Musk has over 20 direct reports at Tesla — here are the ones we know about',\n",
       " 'Trump SPAC shares are now around $16 after hitting $97 earlier this year',\n",
       " \"Here's why U.S. fiscal policy is undermining the Fed's efforts to fight inflation\",\n",
       " \"Why gold and crypto haven't proven to be 'inflation-proof' investments in 2022\"]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headline=[]\n",
    "for i in soup.find_all('a',class_=\"LatestNews-headline\"):\n",
    "    headline.append(i.text)\n",
    "headline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f79cd900",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3 Hours Ago',\n",
       " '4 Hours Ago',\n",
       " '5 Hours Ago',\n",
       " '6 Hours Ago',\n",
       " '6 Hours Ago',\n",
       " '7 Hours Ago',\n",
       " '7 Hours Ago',\n",
       " '7 Hours Ago',\n",
       " '7 Hours Ago',\n",
       " '7 Hours Ago',\n",
       " '8 Hours Ago',\n",
       " '8 Hours Ago',\n",
       " '8 Hours Ago',\n",
       " '9 Hours Ago',\n",
       " '9 Hours Ago',\n",
       " '9 Hours Ago',\n",
       " '12 Hours Ago',\n",
       " '23 Hours Ago',\n",
       " '23 Hours Ago',\n",
       " '23 Hours Ago',\n",
       " '24 Hours Ago',\n",
       " '24 Hours Ago',\n",
       " 'September 23, 2022',\n",
       " 'September 23, 2022',\n",
       " 'September 23, 2022',\n",
       " 'September 23, 2022',\n",
       " 'September 23, 2022',\n",
       " 'September 23, 2022',\n",
       " 'September 23, 2022',\n",
       " 'September 23, 2022']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time=[]\n",
    "for i in soup.find_all('span',class_=\"LatestNews-wrapper\"):\n",
    "    time.append(i.text)\n",
    "time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "05ed02ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cnbc.com/2022/09/24/three-rules-for-a-successful-open-relationship.html',\n",
       " 'https://www.cnbc.com/2022/09/24/biden-administration-awards-1point5-billion-to-fight-opioid-crisis.html',\n",
       " 'https://www.cnbc.com/2022/09/24/top-10-cities-with-the-best-pizzerias-worldwide-.html',\n",
       " 'https://www.cnbc.com/2022/09/24/easy-meals-for-two-under-20-from-black-girls-in-trader-joes-creator.html',\n",
       " 'https://www.cnbc.com/2022/09/24/how-to-raise-resilient-kids-by-developing-their-brains-with-nurturing-routines-parenting-expert.html',\n",
       " 'https://www.cnbc.com/2022/09/24/how-airlines-plan-to-end-one-billion-tons-of-carbon-emissions.html',\n",
       " 'https://www.cnbc.com/2022/09/24/queer-eyes-karamo-brown-shares-morning-routine.html',\n",
       " 'https://www.cnbc.com/2022/09/24/vermont-new-mexico-california-us-states-with-least-air-pollution.html',\n",
       " 'https://www.cnbc.com/2022/09/24/new-york-now-no-1-port-in-us-as-sea-change-in-trade-hits-west-coast.html',\n",
       " 'https://www.cnbc.com/2022/09/24/analysts-name-top-high-conviction-stocks-for-playing-market-turbulence.html',\n",
       " 'https://www.cnbc.com/2022/09/24/feared-stock-market-bottom-retest-is-now-underway.html',\n",
       " 'https://www.cnbc.com/2022/09/24/wallethub-top-10-cities-to-retire-2022-nearly-half-are-in-florida.html',\n",
       " 'https://www.cnbc.com/2022/09/24/quantum-investing-perils-and-promise-of-quantum-computing-are-nearing.html',\n",
       " 'https://www.cnbc.com/2022/09/24/what-parent-plus-borrowers-need-to-know-about-student-loan-forgiveness.html',\n",
       " 'https://www.cnbc.com/2022/09/24/check-in-smoke-up-tune-out-cannabis-friendly-vacation-rentals-catch-on.html',\n",
       " 'https://www.cnbc.com/2022/09/24/convertible-sales-fall-in-us-amid-popularity-of-evs-suvs.html',\n",
       " 'https://www.cnbc.com/2022/09/24/liz-truss-britains-lurch-to-reaganomics-gets-thumbs-down-from-markets.html',\n",
       " 'https://www.cnbc.com/2022/09/23/it-was-another-painful-week-to-own-stocks-but-we-have-a-new-plan-for-monday.html',\n",
       " 'https://www.cnbc.com/2022/09/23/stocks-could-continue-skittish-trading-until-interest-rate-move-calms-down.html',\n",
       " 'https://www.cnbc.com/2022/09/23/pro-picks-watch-all-of-fridays-big-stock-calls-on-cnbc.html',\n",
       " 'https://www.cnbc.com/2022/09/23/careers-where-over-50percent-of-workers-are-happy-with-their-pay.html',\n",
       " 'https://www.cnbc.com/2022/09/23/new-york-ag-wrongly-says-yankees-game-on-apple-tv-costs-extra.html',\n",
       " 'https://www.cnbc.com/2022/09/23/tech-stocks-worst-two-week-stretch-since-the-start-of-pandemic.html',\n",
       " 'https://www.cnbc.com/2022/09/23/jim-cramer-sat-down-with-the-ceos-of-salesforce-and-slack-this-week.html',\n",
       " 'https://www.cnbc.com/2022/09/23/some-homebuyers-are-facing-payment-shock-ways-to-save-on-a-mortgage.html',\n",
       " 'https://www.cnbc.com/2022/09/23/ether-is-down-almost-20percent-since-the-merge-heres-whats-going-on.html',\n",
       " 'https://www.cnbc.com/2022/09/23/elon-musk-direct-reports-at-tesla.html',\n",
       " 'https://www.cnbc.com/2022/09/23/trump-merger-partner-shares-fall-dramatically.html',\n",
       " 'https://www.cnbc.com/2022/09/23/us-fiscal-policy-is-undermining-the-feds-efforts-to-fight-inflation.html',\n",
       " 'https://www.cnbc.com/2022/09/23/why-gold-and-cryptocurrencies-arent-inflation-proof-investments.html']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_link=[]\n",
    "for i in soup.find_all('a', class_=\"LatestNews-headline\"):\n",
    "    news_link.append(i.get('href'))\n",
    "news_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ee3d50",
   "metadata": {},
   "source": [
    "8) Write a python program to scrape the details of most downloaded articles from AI in last 90 days.\n",
    "https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details :\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aae260ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=requests.get('https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles')\n",
    "soup=BeautifulSoup(url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a520830a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Reward is enough',\n",
       " 'Making sense of raw input',\n",
       " 'Law and logic: A review from an argumentation perspective',\n",
       " 'Creativity and artificial intelligence',\n",
       " 'Artificial cognition for social human–robot interaction: An implementation',\n",
       " 'Explanation in artificial intelligence: Insights from the social sciences',\n",
       " 'Making sense of sensory input',\n",
       " 'Conflict-based search for optimal multi-agent pathfinding',\n",
       " 'Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning',\n",
       " 'The Hanabi challenge: A new frontier for AI research',\n",
       " 'Evaluating XAI: A comparison of rule-based and example-based explanations',\n",
       " 'Argumentation in artificial intelligence',\n",
       " 'Algorithms for computing strategies in two-player simultaneous move games',\n",
       " 'Multiple object tracking: A literature review',\n",
       " 'Selection of relevant features and examples in machine learning',\n",
       " 'A survey of inverse reinforcement learning: Challenges, methods and progress',\n",
       " 'Explaining individual predictions when features are dependent: More accurate approximations to Shapley values',\n",
       " 'A review of possible effects of cognitive biases on interpretation of rule-based machine learning models',\n",
       " 'Integrating social power into the decision-making of cognitive agents',\n",
       " \"“That's (not) the output I expected!” On the role of end user expectations in creating explanations of AI systems\",\n",
       " 'Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies',\n",
       " 'Algorithm runtime prediction: Methods & evaluation',\n",
       " 'Wrappers for feature subset selection',\n",
       " 'Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics',\n",
       " 'Quantum computation, quantum theory and AI']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_title=[]\n",
    "for i in soup.find_all('a',class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    paper_title.append(i.text)\n",
    "paper_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1fdafa8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Silver, David, Singh, Satinder, Precup, Doina, Sutton, Richard S. ',\n",
       " 'Evans, Richard, Bošnjak, Matko and 5 more',\n",
       " 'Prakken, Henry, Sartor, Giovanni ',\n",
       " 'Boden, Margaret A. ',\n",
       " 'Lemaignan, Séverin, Warnier, Mathieu and 3 more',\n",
       " 'Miller, Tim ',\n",
       " 'Evans, Richard, Hernández-Orallo, José and 3 more',\n",
       " 'Sharon, Guni, Stern, Roni, Felner, Ariel, Sturtevant, Nathan R. ',\n",
       " 'Sutton, Richard S., Precup, Doina, Singh, Satinder ',\n",
       " 'Bard, Nolan, Foerster, Jakob N. and 13 more',\n",
       " 'van der Waa, Jasper, Nieuwburg, Elisabeth, Cremers, Anita, Neerincx, Mark ',\n",
       " 'Bench-Capon, T.J.M., Dunne, Paul E. ',\n",
       " 'Bošanský, Branislav, Lisý, Viliam and 3 more',\n",
       " 'Luo, Wenhan, Xing, Junliang and 4 more',\n",
       " 'Blum, Avrim L., Langley, Pat ',\n",
       " 'Arora, Saurabh, Doshi, Prashant ',\n",
       " 'Aas, Kjersti, Jullum, Martin, Løland, Anders ',\n",
       " 'Kliegr, Tomáš, Bahník, Štěpán, Fürnkranz, Johannes ',\n",
       " 'Pereira, Gonçalo, Prada, Rui, Santos, Pedro A. ',\n",
       " 'Riveiro, Maria, Thill, Serge ',\n",
       " 'Kenny, Eoin M., Ford, Courtney, Quinn, Molly, Keane, Mark T. ',\n",
       " 'Hutter, Frank, Xu, Lin, Hoos, Holger H., Leyton-Brown, Kevin ',\n",
       " 'Kohavi, Ron, John, George H. ',\n",
       " 'Suchan, Jakob, Bhatt, Mehul, Varadarajan, Srikrishna ',\n",
       " 'Ying, Mingsheng ']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors=[]\n",
    "for i in soup.find_all('span',class_=\"sc-1w3fpd7-0 pgLAT\"):\n",
    "    authors.append(i.text)\n",
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1dc8c0cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['October 2021',\n",
       " 'October 2021',\n",
       " 'October 2015',\n",
       " 'August 1998',\n",
       " 'June 2017',\n",
       " 'February 2019',\n",
       " 'April 2021',\n",
       " 'February 2015',\n",
       " 'August 1999',\n",
       " 'March 2020',\n",
       " 'February 2021',\n",
       " 'October 2007',\n",
       " 'August 2016',\n",
       " 'April 2021',\n",
       " 'December 1997',\n",
       " 'August 2021',\n",
       " 'September 2021',\n",
       " 'June 2021',\n",
       " 'December 2016',\n",
       " 'September 2021',\n",
       " 'May 2021',\n",
       " 'January 2014',\n",
       " 'December 1997',\n",
       " 'October 2021',\n",
       " 'February 2010']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "published_date=[]\n",
    "for i in soup.find_all('span',class_=\"sc-1thf9ly-2 bKddwo\"):\n",
    "    published_date.append(i.text)\n",
    "published_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fd7d418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.sciencedirect.com/science/article/pii/S0004370221000862',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000722',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370215000910',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370298000551',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370216300790',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370218305988',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370220301855',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370214001386',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370299000521',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370219300116',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370220301533',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370207000793',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370216300285',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370220301958',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370297000635',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000515',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000539',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000096',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370216300868',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000588',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000102',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370213001082',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S000437029700043X',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370221000734',\n",
       " 'https://www.sciencedirect.com/science/article/pii/S0004370209001398']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_url=[]\n",
    "for i in soup.find_all('a', class_=\"sc-5smygv-0 nrDZj\"):\n",
    "    paper_url.append(i.get('href'))\n",
    "paper_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7569df4e",
   "metadata": {},
   "source": [
    "9) Write a python program to scrape mentioned details from dineout.co.in :\n",
    "i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7d4e235f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=requests.get('https://www.dineout.co.in/delhi-restaurants/buffet-special')\n",
    "soup=BeautifulSoup(url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "784c88f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Castle Barbeque',\n",
       " 'Jungle Jamboree',\n",
       " 'Castle Barbeque',\n",
       " 'Cafe Knosh',\n",
       " 'The Barbeque Company',\n",
       " 'India Grill',\n",
       " 'Delhi Barbeque',\n",
       " 'The Monarch - Bar Be Que Village',\n",
       " 'Indian Grill Room']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_name=[]\n",
    "for i in soup.find_all('a',class_=\"restnt-name ellipsis\"):\n",
    "    restaurant_name.append(i.text)\n",
    "restaurant_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4a79a28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chinese, North Indian',\n",
       " 'North Indian, Asian, Italian',\n",
       " 'Chinese, North Indian',\n",
       " 'Italian, Continental',\n",
       " 'North Indian, Chinese',\n",
       " 'North Indian, Italian',\n",
       " 'North Indian',\n",
       " 'North Indian',\n",
       " 'North Indian, Mughlai']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuisine=[]\n",
    "for i in soup.find_all('span',class_=\"double-line-ellipsis\"):\n",
    "    cuisine.append(i.text.split( '| ')[1])\n",
    "cuisine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48b9955f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Connaught Place, Central Delhi',\n",
       " '3CS Mall,Lajpat Nagar - 3, South Delhi',\n",
       " 'Pacific Mall,Tagore Garden, West Delhi',\n",
       " 'The Leela Ambience Convention Hotel,Shahdara, East Delhi',\n",
       " 'Gardens Galleria,Sector 38A, Noida',\n",
       " 'Hilton Garden Inn,Saket, South Delhi',\n",
       " 'Taurus Sarovar Portico,Mahipalpur, South Delhi',\n",
       " 'Indirapuram Habitat Centre,Indirapuram, Ghaziabad',\n",
       " 'Suncity Business Tower,Golf Course Road, Gurgaon']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "location=[]\n",
    "for i in soup.find_all('div',class_=\"restnt-loc ellipsis\"):\n",
    "    location.append(i.text)\n",
    "location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0619ab0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['4.1', '3.9', '3.9', '4.3', '4', '3.9', '3.7', '3.8', '4.3']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings=[]\n",
    "for i in soup.find_all('div',class_=\"restnt-rating rating-4\"):\n",
    "    ratings.append(i.text)\n",
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fbc25afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://im1.dineout.co.in/images/uploads/restaurant/sharpen/8/k/b/p86792-16062953735fbe1f4d3fb7e.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/p/m/p59633-166088382462ff137009010.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/j/o/p38113-15959192065f1fcb666130c.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/4/p/m/p406-15438184745c04ccea491bc.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/7/p/k/p79307-16051787755fad1597f2bf9.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/2/v/t/p2687-1482477169585cce712b90f.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/d/i/p52501-1661855212630de5eceb6d2.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/3/n/o/p34822-15599107305cfa594a13c24.jpg?tr=tr:n-medium',\n",
       " 'https://im1.dineout.co.in/images/uploads/restaurant/sharpen/5/y/f/p549-165000147262590640c0afc.jpg?tr=tr:n-medium']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_url=[]\n",
    "for i in soup.find_all('img', class_=\"no-img\"):\n",
    "    image_url.append(i.get('data-src'))\n",
    "image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72603bd",
   "metadata": {},
   "source": [
    "10) Write a python program to scrape the details of top publications from Google Scholar from\n",
    "https://scholar.google.com/citations?view_op=top_venues&hl=en\n",
    "i) Rank\n",
    "ii) Publication\n",
    "iii) h5-index\n",
    "iv) h5-median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9bee5121",
   "metadata": {},
   "outputs": [],
   "source": [
    "url=requests.get('https://scholar.google.com/citations?view_op=top_venues&hl=en')\n",
    "soup=BeautifulSoup(url.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "347a144f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Nature',\n",
       " 'The New England Journal of Medicine',\n",
       " 'Science',\n",
       " 'IEEE/CVF Conference on Computer Vision and Pattern Recognition',\n",
       " 'The Lancet',\n",
       " 'Advanced Materials',\n",
       " 'Nature Communications',\n",
       " 'Cell',\n",
       " 'International Conference on Learning Representations',\n",
       " 'Neural Information Processing Systems',\n",
       " 'JAMA',\n",
       " 'Chemical Reviews',\n",
       " 'Proceedings of the National Academy of Sciences',\n",
       " 'Angewandte Chemie',\n",
       " 'Chemical Society Reviews',\n",
       " 'Journal of the American Chemical Society',\n",
       " 'IEEE/CVF International Conference on Computer Vision',\n",
       " 'Nucleic Acids Research',\n",
       " 'International Conference on Machine Learning',\n",
       " 'Nature Medicine',\n",
       " 'Renewable and Sustainable Energy Reviews',\n",
       " 'Science of The Total Environment',\n",
       " 'Advanced Energy Materials',\n",
       " 'Journal of Clinical Oncology',\n",
       " 'ACS Nano',\n",
       " 'Journal of Cleaner Production',\n",
       " 'Advanced Functional Materials',\n",
       " 'Physical Review Letters',\n",
       " 'Scientific Reports',\n",
       " 'The Lancet Oncology',\n",
       " 'Energy & Environmental Science',\n",
       " 'IEEE Access',\n",
       " 'PLoS ONE',\n",
       " 'Science Advances',\n",
       " 'Journal of the American College of Cardiology',\n",
       " 'Applied Catalysis B: Environmental',\n",
       " 'Nature Genetics',\n",
       " 'BMJ',\n",
       " 'Circulation',\n",
       " 'European Conference on Computer Vision',\n",
       " 'International Journal of Molecular Sciences',\n",
       " 'Nature Materials',\n",
       " 'Chemical engineering journal',\n",
       " 'AAAI Conference on Artificial Intelligence',\n",
       " 'Journal of Materials Chemistry A',\n",
       " 'ACS Applied Materials & Interfaces',\n",
       " 'Nature Biotechnology',\n",
       " 'The Lancet Infectious Diseases',\n",
       " 'Frontiers in Immunology',\n",
       " 'Applied Energy',\n",
       " 'Nano Energy',\n",
       " 'Nature Energy',\n",
       " 'Meeting of the Association for Computational Linguistics (ACL)',\n",
       " 'The Astrophysical Journal',\n",
       " 'Gastroenterology',\n",
       " 'Nature Methods',\n",
       " 'IEEE Transactions on Pattern Analysis and Machine Intelligence',\n",
       " 'Cochrane Database of Systematic Reviews',\n",
       " 'Blood',\n",
       " 'Neuron',\n",
       " 'Nano Letters',\n",
       " 'Morbidity and Mortality Weekly Report',\n",
       " 'European Heart Journal',\n",
       " 'Nature Nanotechnology',\n",
       " 'ACS Catalysis',\n",
       " 'Nature Neuroscience',\n",
       " 'American Economic Review',\n",
       " 'Journal of High Energy Physics',\n",
       " 'IEEE Communications Surveys & Tutorials',\n",
       " 'Annals of Oncology',\n",
       " 'Nutrients',\n",
       " 'Accounts of Chemical Research',\n",
       " 'Immunity',\n",
       " 'Environmental Science & Technology',\n",
       " 'Nature Reviews. Molecular Cell Biology',\n",
       " 'Gut',\n",
       " 'Physical Review D',\n",
       " 'ACS Energy Letters',\n",
       " 'Monthly Notices of the Royal Astronomical Society',\n",
       " 'Conference on Empirical Methods in Natural Language Processing (EMNLP)',\n",
       " 'Clinical Infectious Diseases',\n",
       " 'Cell Metabolism',\n",
       " 'Nature Reviews Immunology',\n",
       " 'Joule',\n",
       " 'Nature Photonics',\n",
       " 'International Journal of Environmental Research and Public Health',\n",
       " 'Environmental Pollution',\n",
       " 'Computers in Human Behavior',\n",
       " 'Frontiers in Microbiology',\n",
       " 'Nature Physics',\n",
       " 'Small',\n",
       " 'Cell Reports',\n",
       " 'Molecular Cell',\n",
       " 'Clinical Cancer Research',\n",
       " 'Bioresource Technology',\n",
       " 'Journal of Business Research',\n",
       " 'Molecular Cancer',\n",
       " 'Sensors',\n",
       " 'Nature Climate Change',\n",
       " 'IEEE Internet of Things Journal']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publication=[]\n",
    "for i in soup.find_all('td',class_=\"gsc_mvt_t\"):\n",
    "    publication.append(i.text)\n",
    "publication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe16c33d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['444',\n",
       " '432',\n",
       " '401',\n",
       " '389',\n",
       " '354',\n",
       " '312',\n",
       " '307',\n",
       " '300',\n",
       " '286',\n",
       " '278',\n",
       " '267',\n",
       " '265',\n",
       " '256',\n",
       " '245',\n",
       " '244',\n",
       " '242',\n",
       " '239',\n",
       " '238',\n",
       " '237',\n",
       " '235',\n",
       " '227',\n",
       " '225',\n",
       " '220',\n",
       " '213',\n",
       " '211',\n",
       " '211',\n",
       " '210',\n",
       " '207',\n",
       " '206',\n",
       " '202',\n",
       " '202',\n",
       " '200',\n",
       " '198',\n",
       " '197',\n",
       " '195',\n",
       " '192',\n",
       " '191',\n",
       " '190',\n",
       " '189',\n",
       " '186',\n",
       " '183',\n",
       " '181',\n",
       " '181',\n",
       " '180',\n",
       " '178',\n",
       " '177',\n",
       " '175',\n",
       " '173',\n",
       " '173',\n",
       " '173',\n",
       " '172',\n",
       " '170',\n",
       " '169',\n",
       " '167',\n",
       " '166',\n",
       " '165',\n",
       " '165',\n",
       " '165',\n",
       " '165',\n",
       " '164',\n",
       " '164',\n",
       " '163',\n",
       " '163',\n",
       " '163',\n",
       " '163',\n",
       " '162',\n",
       " '160',\n",
       " '160',\n",
       " '159',\n",
       " '159',\n",
       " '159',\n",
       " '159',\n",
       " '158',\n",
       " '158',\n",
       " '155',\n",
       " '155',\n",
       " '155',\n",
       " '155',\n",
       " '155',\n",
       " '154',\n",
       " '153',\n",
       " '153',\n",
       " '152',\n",
       " '152',\n",
       " '152',\n",
       " '152',\n",
       " '152',\n",
       " '152',\n",
       " '151',\n",
       " '151',\n",
       " '150',\n",
       " '149',\n",
       " '149',\n",
       " '146',\n",
       " '146',\n",
       " '145',\n",
       " '145',\n",
       " '145',\n",
       " '144',\n",
       " '144']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_index=[]\n",
    "for i in soup.find_all('td',class_=\"gsc_mvt_n\"):\n",
    "    for a in i.find_all('a'):\n",
    "        h5_index.append(a.text)\n",
    "h5_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e451ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['667',\n",
       " '780',\n",
       " '614',\n",
       " '627',\n",
       " '635',\n",
       " '418',\n",
       " '428',\n",
       " '505',\n",
       " '533',\n",
       " '436',\n",
       " '425',\n",
       " '444',\n",
       " '364',\n",
       " '332',\n",
       " '386',\n",
       " '344',\n",
       " '415',\n",
       " '550',\n",
       " '421',\n",
       " '389',\n",
       " '324',\n",
       " '311',\n",
       " '300',\n",
       " '315',\n",
       " '277',\n",
       " '273',\n",
       " '280',\n",
       " '294',\n",
       " '274',\n",
       " '329',\n",
       " '290',\n",
       " '303',\n",
       " '278',\n",
       " '294',\n",
       " '276',\n",
       " '246',\n",
       " '297',\n",
       " '307',\n",
       " '301',\n",
       " '321',\n",
       " '253',\n",
       " '265',\n",
       " '224',\n",
       " '296',\n",
       " '220',\n",
       " '223',\n",
       " '315',\n",
       " '296',\n",
       " '228',\n",
       " '217',\n",
       " '232',\n",
       " '314',\n",
       " '304',\n",
       " '234',\n",
       " '254',\n",
       " '296',\n",
       " '293',\n",
       " '243',\n",
       " '229',\n",
       " '231',\n",
       " '207',\n",
       " '302',\n",
       " '265',\n",
       " '264',\n",
       " '220',\n",
       " '248',\n",
       " '263',\n",
       " '220',\n",
       " '304',\n",
       " '243',\n",
       " '214',\n",
       " '211',\n",
       " '242',\n",
       " '214',\n",
       " '340',\n",
       " '235',\n",
       " '217',\n",
       " '212',\n",
       " '194',\n",
       " '249',\n",
       " '278',\n",
       " '211',\n",
       " '292',\n",
       " '233',\n",
       " '228',\n",
       " '225',\n",
       " '222',\n",
       " '214',\n",
       " '225',\n",
       " '222',\n",
       " '196',\n",
       " '205',\n",
       " '202',\n",
       " '201',\n",
       " '190',\n",
       " '233',\n",
       " '209',\n",
       " '201',\n",
       " '228',\n",
       " '212']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5_median=[]\n",
    "for i in soup.find_all('td',class_=\"gsc_mvt_n\"):\n",
    "    for a in i.find_all('span',class_=\"gs_ibl gsc_mp_anchor\"):\n",
    "        h5_median.append(a.text)\n",
    "h5_median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59934dc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd14c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
